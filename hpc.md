# RCS High Performance Computing \(HPC\) Documentation

## Contents

1. [Introduction](getting-started/introduction.md)
2. [Logging In](getting-started/logging-in.md)
3. [Available Filesystems](available-filesystems/available-filesystems.md)
4. [Batch System Overview](using-batch/batch-overview.md)
5. [Third-Party Software](third-party-software/third-party-software.md)
6. [Compiling from Source Code](compiling-source/compiling-from-source-code.md)
7. [System Architecture](system-architecture/system-architecture.md)
8. [Community Condo Model](community-condo-model/community-condo-model.md)
9. [Policies](policies/policies.md)

## Chinook

In 2015, the Geophysical Institute launched Chinook as an energy efficient linux cluster purchased from [Penguin Computing, Inc](http://www.penguincomputing.com/). Chinook is named in honor of long time GI colleague Kevin Engle's unique, strong, collaborative nature and passion for salmon and Alaska. Chinook is made possible by the Geophysical Institute, IARC, the Institute of Arctic Biology, the Vice Chancellor of Research, and your fellow colleagues who contribute shares. In 2016 and 2017, Chinook expansions were also supported with funding from the M. J. Murdock Charitable Trust.

Chinook is the foundation for an energy-efficient, condo-style HPC cluster for UA researchers. The computing environment hosted on Chinook includes:

* 67x Relion 1900 Compute Nodes each with dual Intel Xeon 14-core processors \(28 cores per node\) and 128 GB memory
* 38x Relion XE1112 Compute Nodes each with dual Intel Xeon Scalable 20-core processor \(40 cores per node\) and 192 GBs memory
* 13x Relion XE1312 Compute Nodes each with dual Intel Xeon 24-core processors \(48 cores per node\) and 256 GBs memory
* 3x Relion 1900 Compute Nodes each with dual Intel Xeon 14-core processors \(28 cores per node\) and 1.5 TB memory
* 5x Altus XE2318GT-AIR Compute Nodes each with dual AMD EPYC 24-core processors \(48 cores per node\), 1 TB memory, and 8x NVIDIA L40S GPU accelerators
* 1x Altus XE5318 Compute Node with dual AMD EPYC 24-core processors \(48 cores per node\), 1 TB memory, and 8x NVIDIA H100 GPU accelerators
* Multiple login nodes with dual Intel Xeon processors and 48 or more GBs memory
* CentOS operating system, Slurm open-source workload management software, and Scyld ClusterWare HPC management software
* Compute and login node access to the 307 TB Lustre scratch file system

Are you interested in using the Chinook HPC cluster in your computational work? Please read our directions on how to [obtain RCS project and user accounts](https://www.gi.alaska.edu/research-computing-systems/getting-access).

## XSEDE/Campus Champions

Members of RCS staff are part of [XSEDE's Campus Champions program](https://www.xsede.org/community-engagement/campus-champions), providing support and contacts for XSEDE systems and XSEDE startup allocations. Please contact RCS if you are interested in pursuing XSEDE resources.

